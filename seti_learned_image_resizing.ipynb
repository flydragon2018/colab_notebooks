{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seti-learned-image-resizing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZft8_SlTBe-"
      },
      "source": [
        "# About \n",
        "\n",
        "When training CNNs, we typically resize our input images to some standard size before passing them to the model. For example, if the original images are `600 x 400` we might resize them to a resolution like `248 x 248` or `512 x 512` as a preprocessing step.\n",
        "\n",
        "But this resizing results in information loss, and every time we resize images with `opencv`, we rely on some interpolation algorithm to decrease this resize-loss as much as possible. Instead of using `cv2.INTER_LINEAR` or `cv2.INTER_AREA`, why not train our own 'algorithm' to resize the images? The simplest example would be to take a `1024 x 1024` image and convolve it to size `512 x 512` before giving it to the CNN backbone. This way, the model learns how to best resize the images alongside the main training task. \n",
        "\n",
        "This was done succesfully [here](https://www.kaggle.com/c/ranzcr-clip-catheter-line-classification/discussion/226557) and [here](https://www.kaggle.com/c/understanding_cloud_organization/discussion/118255) - two competitions in which the native image resolutions where > `1400 x 1400`. Seeing as we have fairly large 'images' for this competition, let's try it here. Instead of using either of the two above approaches, I want to try to implement the approach from [this paper](https://arxiv.org/abs/2103.09950) in which the proposed 'learned image resizer' looks like:\n",
        "\n",
        "![Capture.PNG](attachment:ccf6a3fd-9ee4-4cff-b6a6-4fd3d1588735.PNG)\n",
        "\n",
        "In this commit, we will feed the learned image resizer original resolution images for it to resize to `256 x 256`. Note that I am not 100% sure I coded all this correctly, so if you see anything suspicious, please let me know. While I opted for the approach taken in the paper, the previously linked approaches seem fruitful as well and require less training time. \n",
        "\n",
        "Code template is taken from the one and only [yasufuminakama](https://www.kaggle.com/yasufuminakama)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M0wA5mmTBfC"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FaQ8TNYeG7C",
        "outputId": "5af2cb4e-5c2e-457e-d4c9-a2ae7219c136"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Aug 12 09:37:27 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxOfb9RSTsF2",
        "outputId": "19d93e04-a6db-466d-82b2-b35901f7fcf1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D416T0AhTsK4"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0omyySSTsOY"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkL24OolEXYR",
        "outputId": "b0fa80f8-1a2b-44d6-e2a8-a90d8f33051c"
      },
      "source": [
        "!pip install gcsfs"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.7/dist-packages (2021.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gcsfs) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs) (0.4.4)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (1.32.1)\n",
            "Requirement already satisfied: fsspec==2021.07.0 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (2021.7.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gcsfs) (3.7.4.post0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (4.7.2)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (57.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (4.2.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (5.1.0)\n",
            "Requirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (3.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (21.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (3.7.4.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (1.6.3)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (3.0.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->gcsfs) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (2021.5.30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "QqKDyYIjEkPB",
        "outputId": "b57e8266-09ab-49eb-c4ef-aa48e4bf64a1"
      },
      "source": [
        "'''\n",
        "import os \n",
        "import gcsfs\n",
        "import google.auth\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "credentials, project_id = google.auth.default()\n",
        "fs = gcsfs.GCSFileSystem(project=project_id, token=credentials)\n",
        "\n",
        "use_tpu = True #@param {type:\"boolean\"}\n",
        "bucket = 'vocab_jb'\n",
        "\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "reader = fs.open(\"gs://your-bucket-here/kinglear_on_roids.txt\")\n",
        "for text in reader:\n",
        "  print(text)\n",
        "\n",
        "'''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimport os \\nimport gcsfs\\nimport google.auth\\nfrom google.colab import auth\\nauth.authenticate_user()\\n\\ncredentials, project_id = google.auth.default()\\nfs = gcsfs.GCSFileSystem(project=project_id, token=credentials)\\n\\nuse_tpu = True #@param {type:\"boolean\"}\\nbucket = \\'vocab_jb\\'\\n\\nif use_tpu:\\n    assert \\'COLAB_TPU_ADDR\\' in os.environ, \\'Missing TPU; did you request a TPU in Notebook Settings?\\'\\n\\n%tensorflow_version 2.x\\nimport tensorflow as tf\\nprint(\"Tensorflow version \" + tf.__version__)\\n\\ntry:\\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver(\\'grpc://\\' + os.environ[\\'COLAB_TPU_ADDR\\'])  # TPU detection\\n  print(\\'Running on TPU \\', tpu.cluster_spec().as_dict()[\\'worker\\'])\\nexcept ValueError:\\n  raise BaseException(\\'ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!\\')\\n\\ntf.config.experimental_connect_to_cluster(tpu)\\ntf.tpu.experimental.initialize_tpu_system(tpu)\\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\\n\\nreader = fs.open(\"gs://your-bucket-here/kinglear_on_roids.txt\")\\nfor text in reader:\\n  print(text)\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDVvo_bNFK8U"
      },
      "source": [
        "import gcsfs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ztkX2jfEkTJ"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeMYr6enTBfD"
      },
      "source": [
        "! pip install -q pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIUzMWGNTBfE",
        "outputId": "b105891a-33f1-40fe-8bbd-b81cb5717973"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc\n",
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9.0+cu102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_5J8-u6THPq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a27155ac-fd1d-49d9-b615-dd83c25eed9c"
      },
      "source": [
        "'''\n",
        "seti='gs://kds-03e1a10468eed52cf1f7962028d75ca9e9ca67e4b3c30a4f640075f4'\n",
        "fmix =    'gs://kds-b5f21f33e9eb8e0e63f3dd7434f3ed120f0218e9ebe53b80abb0b459'\n",
        "pmodel=              'gs://kds-81e492228f9bcc00922cc4da75d30ab287cdd573da99e685e1531aa5'\n",
        "\n",
        "'''"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nseti='gs://kds-03e1a10468eed52cf1f7962028d75ca9e9ca67e4b3c30a4f640075f4'\\nfmix =    'gs://kds-b5f21f33e9eb8e0e63f3dd7434f3ed120f0218e9ebe53b80abb0b459'\\npmodel=              'gs://kds-81e492228f9bcc00922cc4da75d30ab287cdd573da99e685e1531aa5'\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEwngDcCTHT4"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY5WJgd7UIQ2"
      },
      "source": [
        "#  !gsutil -m cp -r  gs://kds-03e1a10468eed52cf1f7962028d75ca9e9ca67e4b3c30a4f640075f4/train/ -d /content/drive/MyDrive/kds-03e1a10468eed52cf1f7962028d75ca9e9ca67e4b3c30a4f640075f4/"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmuhN87jUIVR"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkcdpeOUXYix",
        "outputId": "3a44e25f-31d8-4bb5-b47b-639919f4d133"
      },
      "source": [
        "!gsutil cp  gs://kds-03e1a10468eed52cf1f7962028d75ca9e9ca67e4b3c30a4f640075f4/*.csv /content/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://kds-03e1a10468eed52cf1f7962028d75ca9e9ca67e4b3c30a4f640075f4/sample_submission.csv...\n",
            "Copying gs://kds-03e1a10468eed52cf1f7962028d75ca9e9ca67e4b3c30a4f640075f4/train_labels.csv...\n",
            "/ [2 files][  1.8 MiB/  1.8 MiB]                                                \n",
            "Operation completed over 2 objects/1.8 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts185wTUXYmZ"
      },
      "source": [
        "#!gsutil cp -r  gs://kds-b5f21f33e9eb8e0e63f3dd7434f3ed120f0218e9ebe53b80abb0b459 /content/drive/MyDrive/"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ELH0qszbxgS"
      },
      "source": [
        "#!gsutil cp -r gs://kds-81e492228f9bcc00922cc4da75d30ab287cdd573da99e685e1531aa5 /content/drive/MyDrive/"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEAIHj-DbxmG"
      },
      "source": [
        "seti='/content/drive/MyDrive/kds-03e1a10468eed52cf1f7962028d75ca9e9ca67e4b3c30a4f640075f4/'\n",
        "fmix =    '/content/drive/MyDrive/kds-b5f21f33e9eb8e0e63f3dd7434f3ed120f0218e9ebe53b80abb0b459'\n",
        "pmodel=              '/content/drive/MyDrive/kds-81e492228f9bcc00922cc4da75d30ab287cdd573da99e685e1531aa5'"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "7dIuH1z6TBfE",
        "outputId": "50ab4f35-16ac-485f-9908-75b7d8472f2f"
      },
      "source": [
        "train = pd.read_csv('train_labels.csv')\n",
        "test = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "def get_train_file_path(image_id):\n",
        "    return seti+\"/train/{}/{}.npy\".format(image_id[0], image_id)\n",
        "\n",
        "def get_test_file_path(image_id):\n",
        "    return seti+\"/test/{}/{}.npy\".format(image_id[0], image_id)\n",
        "\n",
        "train['file_path'] = train['id'].apply(get_train_file_path)\n",
        "test['file_path'] = test['id'].apply(get_test_file_path)\n",
        "\n",
        "display(train.head())\n",
        "display(test.head())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>file_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000799a2b2c42d</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/drive/MyDrive/kds-03e1a10468eed52cf1f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00042890562ff68</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/drive/MyDrive/kds-03e1a10468eed52cf1f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0005364cdcb8e5b</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/drive/MyDrive/kds-03e1a10468eed52cf1f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0007a5a46901c56</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/drive/MyDrive/kds-03e1a10468eed52cf1f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0009283e145448e</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/drive/MyDrive/kds-03e1a10468eed52cf1f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                id  target                                          file_path\n",
              "0  0000799a2b2c42d       0  /content/drive/MyDrive/kds-03e1a10468eed52cf1f...\n",
              "1  00042890562ff68       0  /content/drive/MyDrive/kds-03e1a10468eed52cf1f...\n",
              "2  0005364cdcb8e5b       0  /content/drive/MyDrive/kds-03e1a10468eed52cf1f...\n",
              "3  0007a5a46901c56       0  /content/drive/MyDrive/kds-03e1a10468eed52cf1f...\n",
              "4  0009283e145448e       0  /content/drive/MyDrive/kds-03e1a10468eed52cf1f..."
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>file_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000bf832cae9ff1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>/content/drive/MyDrive/kds-03e1a10468eed52cf1f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000c74cc71a1140</td>\n",
              "      <td>0.5</td>\n",
              "      <td>/content/drive/MyDrive/kds-03e1a10468eed52cf1f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000f5f9851161d3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>/content/drive/MyDrive/kds-03e1a10468eed52cf1f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000f7499e95aba6</td>\n",
              "      <td>0.5</td>\n",
              "      <td>/content/drive/MyDrive/kds-03e1a10468eed52cf1f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00133ce6ec257f9</td>\n",
              "      <td>0.5</td>\n",
              "      <td>/content/drive/MyDrive/kds-03e1a10468eed52cf1f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                id  target                                          file_path\n",
              "0  000bf832cae9ff1     0.5  /content/drive/MyDrive/kds-03e1a10468eed52cf1f...\n",
              "1  000c74cc71a1140     0.5  /content/drive/MyDrive/kds-03e1a10468eed52cf1f...\n",
              "2  000f5f9851161d3     0.5  /content/drive/MyDrive/kds-03e1a10468eed52cf1f...\n",
              "3  000f7499e95aba6     0.5  /content/drive/MyDrive/kds-03e1a10468eed52cf1f...\n",
              "4  00133ce6ec257f9     0.5  /content/drive/MyDrive/kds-03e1a10468eed52cf1f..."
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKCJPhw-TBfF"
      },
      "source": [
        "# Directory settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk_kX8uoTBfF"
      },
      "source": [
        "# ====================================================\n",
        "# Directory settings\n",
        "# ====================================================\n",
        "import os\n",
        "\n",
        "OUTPUT_DIR = './'\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3BjiSa3TBfH"
      },
      "source": [
        "# CFG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCrI6jtuTBfH"
      },
      "source": [
        "# ====================================================\n",
        "# CFG\n",
        "# ====================================================\n",
        "class CFG:\n",
        "    debug=False\n",
        "    #debug=True\n",
        "    print_freq=100\n",
        "    num_workers=8\n",
        "    #model_name='efficientnet_b0'  #'efficientnet_b0', 'vit_base_patch16_224', 'tf_efficientnet_b4_ns'\n",
        "    model_name='efficientnet_b2'\n",
        "    input_size=512   #512, 768, 1028, 'original'\n",
        "    output_size=256 \n",
        "    scheduler='CosineAnnealingLR' #'ReduceLROnPlateau', 'CosineAnnealingLR'\n",
        "    #epochs=18\n",
        "    epochs=12\n",
        "    factor=0.2 # ReduceLROnPlateau\n",
        "    patience=4 # ReduceLROnPlateau\n",
        "    eps=1e-6 # ReduceLROnPlateau\n",
        "    T_max=15 # CosineAnnealingLR\n",
        "    lr=1e-4\n",
        "    min_lr=1e-6\n",
        "    batch_size=32\n",
        "    optimizer='adamw'   #adamw', 'adam' \n",
        "    weight_decay=1e-6\n",
        "    gradient_accumulation_steps=1\n",
        "    max_grad_norm=1100\n",
        "    seed=29\n",
        "    target_size=1\n",
        "    target_col='target'\n",
        "    n_fold=5\n",
        "    trn_folds=[0]\n",
        "    train=True\n",
        "    mode='spatial_3'   #'channel_3', 'channel_6', 'spatial_3', spatial_6'\n",
        "    aug_mode='mixup'   #'mixup', 'fmix'\n",
        "    warmup_epochs=2\n",
        "    multiplier=10\n",
        "    epoch_no_aug=0\n",
        "    \n",
        "if CFG.debug:\n",
        "    CFG.epochs=10\n",
        "    train=train.sample(n=1000, random_state=CFG.seed).reset_index(drop=True)   "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZcsnSw3TBfJ"
      },
      "source": [
        "# Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRi-Mwcte3Ft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c90232a-292e-4f33-e36d-d3814b6e4511"
      },
      "source": [
        "!pip install albumentations==1.0.1\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: albumentations==1.0.1 in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.1) (3.13)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.1) (0.16.2)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.1) (4.5.3.56)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.1) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.1) (1.19.5)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.1) (3.2.2)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.1) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.1) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.1) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.1) (2.5.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.1) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.1) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.1) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.1) (1.15.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations==1.0.1) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaEHYjO2e3Le"
      },
      "source": [
        "\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import albumentations "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "q8-WyEvhPqNG",
        "outputId": "9f1aeaa6-ba3e-428f-e703-56b1513a5834"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-la23JzPh2R"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRX_cK-oOzuf"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9bVHqlwTBfJ"
      },
      "source": [
        "# ====================================================\n",
        "# Library\n",
        "# ====================================================\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/kds-b5f21f33e9eb8e0e63f3dd7434f3ed120f0218e9ebe53b80abb0b459/FMix-master')\n",
        "sys.path.append('/content/drive/MyDrive/kds-81e492228f9bcc00922cc4da75d30ab287cdd573da99e685e1531aa5/pytorch-image-models-master')\n",
        "from fmix import sample_mask\n",
        "\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from contextlib import contextmanager\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from functools import partial\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, AdamW\n",
        "import torchvision.models as models\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import albumentations as A\n",
        "from albumentations import ImageOnlyTransform\n",
        "\n",
        "import timm\n",
        "from warmup_scheduler import GradualWarmupScheduler\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsFt6KKDTBfK"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCZzV6p9TBfK"
      },
      "source": [
        "# ====================================================\n",
        "# Utils\n",
        "# ====================================================\n",
        "def get_score(y_true, y_pred):\n",
        "    score = roc_auc_score(y_true, y_pred)\n",
        "    return score\n",
        "\n",
        "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
        "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
        "    logger = getLogger(__name__)\n",
        "    logger.setLevel(INFO)\n",
        "    handler1 = StreamHandler()\n",
        "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
        "    handler2 = FileHandler(filename=log_file)\n",
        "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
        "    logger.addHandler(handler1)\n",
        "    logger.addHandler(handler2)\n",
        "    return logger\n",
        "\n",
        "LOGGER = init_logger()\n",
        "\n",
        "def seed_torch(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_torch(seed=CFG.seed)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD4_MBm7TBfL"
      },
      "source": [
        "# CV split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx80-P-ZTBfL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "7d2e00c4-9439-48f6-a658-39e8a8629586"
      },
      "source": [
        "Fold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
        "for n, (train_index, val_index) in enumerate(Fold.split(train, train[CFG.target_col])):\n",
        "    train.loc[val_index, 'fold'] = int(n)\n",
        "train['fold'] = train['fold'].astype(int)\n",
        "display(train.groupby(['fold', 'target']).size())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "fold  target\n",
              "0     0         10800\n",
              "      1          1200\n",
              "1     0         10800\n",
              "      1          1200\n",
              "2     0         10800\n",
              "      1          1200\n",
              "3     0         10800\n",
              "      1          1200\n",
              "4     0         10800\n",
              "      1          1200\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCJ-MNm0TBfL"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD-shXwGTBfL"
      },
      "source": [
        "# ====================================================\n",
        "# Dataset\n",
        "# ====================================================\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, df, mode=CFG.mode, transform=None):\n",
        "        self.df = df\n",
        "        self.file_names = df['file_path'].values\n",
        "        self.labels = df[CFG.target_col].values\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_names[idx]\n",
        "        label = self.labels[idx]\n",
        "        #image = np.load(file_path)\n",
        "        image = np.load(file_path)\n",
        "        if self.mode in ['spatial_3', 'channel_3']:\n",
        "            image = image[::2]\n",
        "            image = image.astype(np.float32)\n",
        "        else:\n",
        "            image = image.astype(np.float32)\n",
        "        if self.mode in ['spatial_3', 'spatial_6']: \n",
        "            image = np.vstack(image).transpose((1, 0))\n",
        "        elif self.mode in ['channel_3', 'channel_6']:\n",
        "            #image = np.transpose(image, (1,2,0))\n",
        "            image = np.transpose(image, [1,2,0])\n",
        "            \n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)['image']\n",
        "        else:\n",
        "            image = image[np.newaxis,:,:]\n",
        "            image = torch.from_numpy(image).float()\n",
        "            \n",
        "        label = torch.tensor(label).float()\n",
        "        return image, label\n",
        "    \n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, df, mode=CFG.mode, transform=None):\n",
        "        self.df = df\n",
        "        self.file_names = df['file_path'].values\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_names[idx]\n",
        "        image = np.load(file_path)\n",
        "        if self.mode in ['spatial_3', 'channel_3']:\n",
        "            image = image[::2].astype(np.float32)\n",
        "        else:\n",
        "            image = image.astype(np.float32)\n",
        "        if self.mode in ['spatial_3', 'spatial_6']:\n",
        "            image = np.vstack(image).transpose((1, 0))\n",
        "        elif self.mode in ['channel_3', 'channel_6']:\n",
        "            #image = np.transpose(image, (1,2,0))\n",
        "            image = np.transpose(image, [1,2,0])\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)['image']\n",
        "        else:\n",
        "            image = image[np.newaxis,:,:]\n",
        "            image = torch.from_numpy(image).float()\n",
        "        return image\n",
        "    \n",
        "def worker_init_fn(worker_id):                                                          \n",
        "    np.random.seed(np.random.get_state()[1][0] + worker_id)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So5mbuwGTBfM"
      },
      "source": [
        "# Transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO0d9U0pTBfM"
      },
      "source": [
        "# ====================================================\n",
        "# Transforms\n",
        "# ====================================================\n",
        "def get_transforms(*, data):\n",
        "    if data == 'train':\n",
        "        if type(CFG.input_size) == int:\n",
        "            return A.Compose([\n",
        "                   A.Resize(CFG.input_size, CFG.input_size),\n",
        "                   A.HorizontalFlip(p=.5),\n",
        "                   A.VerticalFlip(p=.5),\n",
        "                   A.ShiftScaleRotate(rotate_limit=0, p=.25),\n",
        "                   A.MotionBlur(p=.2),\n",
        "                   A.IAASharpen(p=.25),\n",
        "                   ToTensorV2(),\n",
        "            ])\n",
        "        else:\n",
        "            return A.Compose([\n",
        "                   A.HorizontalFlip(p=.5),\n",
        "                   A.VerticalFlip(p=.5),\n",
        "                   A.ShiftScaleRotate(rotate_limit=0, p=.25),\n",
        "                   A.MotionBlur(p=.2),\n",
        "                   A.IAASharpen(p=.25),\n",
        "                   ToTensorV2(),\n",
        "            ])\n",
        "            \n",
        "    elif data == 'valid':\n",
        "        if type(CFG.input_size) == int:\n",
        "            return A.Compose([\n",
        "                   A.Resize(CFG.input_size, CFG.input_size),\n",
        "                   ToTensorV2(),\n",
        "            ])\n",
        "        else:\n",
        "            return A.Compose([\n",
        "                   ToTensorV2(),\n",
        "            ])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifKt3qMSTBfN"
      },
      "source": [
        "# https://github.com/facebookresearch/mixup-cifar10/blob/master/train.py\n",
        "def mixup_data(x, y, alpha=1, use_cuda=True):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size()[0]\n",
        "    if use_cuda:\n",
        "        index = torch.randperm(batch_size).cuda()\n",
        "    else:\n",
        "        index = torch.randperm(batch_size)\n",
        "    mixed_x = (lam**.5) * x + ((1 - lam)**.5) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def fmix_data(data, targets, alpha=1.0, \n",
        "              decay_power=3.0,\n",
        "              max_soft=0.0,\n",
        "              shape=(CFG.input_size, CFG.input_size),):\n",
        "    lam, mask = sample_mask(alpha, decay_power, shape, max_soft)\n",
        "    indices = torch.randperm(data.size(0)).cuda()\n",
        "    shuffled_data = data[indices]\n",
        "    \n",
        "    targets_a = targets\n",
        "    targets_b = targets[indices]\n",
        "    x1 = torch.from_numpy(mask).float()*data\n",
        "    x2 = torch.from_numpy(1-mask).float()*shuffled_data\n",
        "    return (x1+x2), targets_a, targets_b, lam\n",
        "\n",
        "def aug_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ2MmSYrTBfN"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv0e9JNkTBfN"
      },
      "source": [
        "# ====================================================\n",
        "# Model\n",
        "# ====================================================\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, cfg, pretrained=False):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.n = 16\n",
        "        self.slope = .1\n",
        "        self.r = 1\n",
        "        \n",
        "        if self.cfg.mode in ['spatial_3', 'spatial_6']: \n",
        "            self.cnn = timm.create_model(self.cfg.model_name, pretrained=pretrained, in_chans=1)\n",
        "        elif self.cfg.mode == 'channel_3':\n",
        "            self.cnn = timm.create_model(self.cfg.model_name, pretrained=pretrained, in_chans=3)\n",
        "        elif self.cfg.mode == 'channel_6':\n",
        "            self.cnn = timm.create_model(self.cfg.model_name, pretrained=pretrained, in_chans=6)\n",
        "        if hasattr(self.cnn, \"fc\"):\n",
        "            nb_ft = self.cnn.fc.in_features\n",
        "            self.cnn.fc = nn.Identity()\n",
        "        elif hasattr(self.cnn, \"_fc\"):\n",
        "            nb_ft = self.cnn._fc.in_features\n",
        "            self.cnn._fc = nn.Identity()\n",
        "        elif hasattr(self.cnn, \"classifier\"):\n",
        "            nb_ft = self.cnn.classifier.in_features\n",
        "            self.cnn.classifier = nn.Identity()\n",
        "        elif hasattr(self.cnn, \"last_linear\"):\n",
        "            nb_ft = self.cnn.last_linear.in_features\n",
        "            self.cnn.last_linear = nn.Identity()\n",
        "        elif hasattr(self.cnn, \"head\"):\n",
        "            nb_ft = self.cnn.head.in_features\n",
        "            self.cnn.head = nn.Identity()\n",
        "        \n",
        "        self.block1 = nn.Sequential(\n",
        "                nn.Conv2d(1, self.n, kernel_size=(7, 7), stride=(1,1), padding=(1, 1), bias=False),\n",
        "                nn.LeakyReLU(negative_slope=self.slope),\n",
        "                nn.Conv2d(self.n, self.n, kernel_size=(1, 1), stride=(1,1), padding=(1, 1), bias=False),\n",
        "                nn.LeakyReLU(negative_slope=self.slope),\n",
        "                nn.BatchNorm2d(self.n))\n",
        "        self.block2 = nn.Sequential(\n",
        "                nn.Conv2d(self.n, self.n, kernel_size=(3, 3), stride=(1,1), padding=(1, 1), bias=False),\n",
        "                nn.BatchNorm2d(self.n),\n",
        "                nn.LeakyReLU(negative_slope=self.slope),\n",
        "                nn.Conv2d(self.n, self.n, kernel_size=(3, 3), stride=(1,1), padding=(1, 1), bias=False),\n",
        "                nn.BatchNorm2d(self.n))\n",
        "        self.block3 = nn.Sequential(\n",
        "                nn.Conv2d(self.n, self.n, kernel_size=(3, 3), stride=(1,1), padding=(1, 1), bias=False),\n",
        "                nn.BatchNorm2d(self.n))\n",
        "        self.block4 = nn.Sequential(\n",
        "                nn.Conv2d(self.n, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False))\n",
        "        self.fc = nn.Linear(nb_ft, self.cfg.target_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res1 = F.interpolate(x, size=(self.cfg.output_size, self.cfg.output_size), mode='bilinear')\n",
        "        x = self.block1(x)\n",
        "        res2 = F.interpolate(x, size=(self.cfg.output_size, self.cfg.output_size), mode='bilinear')\n",
        "\n",
        "        x = self.block2(res2)\n",
        "        x += res2\n",
        "        if self.r > 1:\n",
        "            for _ in range(self.r):\n",
        "                res2 = x\n",
        "                x = self.block2(x)\n",
        "                x += res2\n",
        " \n",
        "        x = self.block3(x)\n",
        "        x += res2\n",
        "        \n",
        "        x = self.block4(x)\n",
        "        x += res1\n",
        "        \n",
        "        x = self.cnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG_DCZaUTBfP"
      },
      "source": [
        "# Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKKGZw_gTBfS"
      },
      "source": [
        "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
        "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
        "        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n",
        "    def get_lr(self):\n",
        "        if self.last_epoch > self.total_epoch:\n",
        "            if self.after_scheduler:\n",
        "                if not self.finished:\n",
        "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
        "                    self.finished = True\n",
        "                return self.after_scheduler.get_lr()\n",
        "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
        "        if self.multiplier == 1.0:\n",
        "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvHMlzyTTBfS"
      },
      "source": [
        "# Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHdF8owxTBfT"
      },
      "source": [
        "# ====================================================\n",
        "# Helper functions\n",
        "# ====================================================\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, mode):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    scores = AverageMeter()\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    start = end = time.time()\n",
        "    global_step = 0\n",
        "    for step, (images, labels) in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "        if mode == 'mixup':\n",
        "            images, targets_a, targets_b, lam = mixup_data(images, labels.view(-1, 1), use_cuda=True)\n",
        "            images = images.to(device)\n",
        "            targets_a = targets_a.to(device)\n",
        "            targets_b = targets_b.to(device)\n",
        "            y_preds = model(images)\n",
        "            loss = aug_criterion(criterion, y_preds, targets_a, targets_b, lam)\n",
        "            \n",
        "        elif mode == 'fmix':\n",
        "            images, targets_a, targets_b, lam = fmix_data(images, labels.view(-1, 1))\n",
        "            images = images.to(device)\n",
        "            targets_a = targets_a.to(device)\n",
        "            targets_b = targets_b.to(device)\n",
        "            y_preds = model(images)\n",
        "            loss = aug_criterion(criterion, y_preds, targets_a, targets_b, lam)\n",
        "\n",
        "        else:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            y_preds = model(images)\n",
        "            loss = criterion(y_preds.view(-1), labels)\n",
        "            \n",
        "        batch_size = labels.size(0)\n",
        "        \n",
        "        # record loss\n",
        "        losses.update(loss.item(), batch_size)\n",
        "        if CFG.gradient_accumulation_steps > 1:\n",
        "            loss = loss / CFG.gradient_accumulation_steps\n",
        "        else:\n",
        "            loss.backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
        "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
        "            print('Epoch: [{0}][{1}/{2}] '\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
        "                  'Elapsed {remain:s} '\n",
        "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
        "                  'Grad: {grad_norm:.4f}  '\n",
        "                  #'LR: {lr:.6f}  '\n",
        "                  .format(\n",
        "                   epoch+1, step, len(train_loader), batch_time=batch_time,\n",
        "                   data_time=data_time, loss=losses,\n",
        "                   remain=timeSince(start, float(step+1)/len(train_loader)),\n",
        "                   grad_norm=grad_norm,\n",
        "                   #lr=scheduler.get_lr()[0],\n",
        "                   ))\n",
        "    return losses.avg\n",
        "\n",
        "def valid_fn(valid_loader, model, criterion, device):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    scores = AverageMeter()\n",
        "    # switch to evaluation mode\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    start = end = time.time()\n",
        "    for step, (images, labels) in enumerate(valid_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch_size = labels.size(0)\n",
        "        # compute loss\n",
        "        with torch.no_grad():\n",
        "            y_preds = model(images)\n",
        "        \n",
        "        loss = criterion(y_preds.view(-1), labels)\n",
        "        losses.update(loss.item(), batch_size)\n",
        "        # record accuracy\n",
        "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
        "        if CFG.gradient_accumulation_steps > 1:\n",
        "            loss = loss / CFG.gradient_accumulation_steps\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
        "            print('EVAL: [{0}/{1}] '\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
        "                  'Elapsed {remain:s} '\n",
        "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
        "                  .format(\n",
        "                   step, len(valid_loader), batch_time=batch_time,\n",
        "                   data_time=data_time, loss=losses,\n",
        "                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
        "                   ))\n",
        "    predictions = np.concatenate(preds)\n",
        "    return losses.avg, predictions\n",
        "\n",
        "def flip_inference(models, test_loader, device):\n",
        "    preds = []\n",
        "    for i, fold in enumerate(CFG.trn_folds):\n",
        "        model = models[i]\n",
        "        probs = []\n",
        "        for step, (images) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
        "            images = images.to(device)\n",
        "            with torch.no_grad():\n",
        "                y_preds1 = model(images)\n",
        "                y_preds2 = model(images.flip(-1))\n",
        "                y_preds3 = model(images.flip(-2))\n",
        "                y_preds4 = model(images.flip(-1).flip(-2))\n",
        "            y_preds = (y_preds1.sigmoid().to('cpu').numpy() + y_preds2.sigmoid().to('cpu').numpy() + y_preds3.sigmoid().to('cpu').numpy() + y_preds4.sigmoid().to('cpu').numpy()) / 4\n",
        "            probs.append(y_preds[:, 0])\n",
        "        preds.append(np.concatenate(probs))\n",
        "    return preds\n",
        "\n",
        "def get_models(CFG, mode='loss'):\n",
        "    models = []\n",
        "    for fold in CFG.trn_folds:\n",
        "        model = CustomModel(cfg=CFG, pretrained=False)\n",
        "        path = OUTPUT_DIR + f'{CFG.model_name}_fold{fold}_best_{mode}.pth'\n",
        "        model.load_state_dict(torch.load(path)['model'])\n",
        "        model.eval()\n",
        "        model.to(device)\n",
        "        models.append(model)\n",
        "    return models"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf-_3sl8TBfU"
      },
      "source": [
        "# Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU9Au3sHTBfV"
      },
      "source": [
        "def train_loop(folds, fold):\n",
        "    \n",
        "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
        "    # ====================================================\n",
        "    # loader\n",
        "    # ====================================================\n",
        "    trn_idx = folds[folds['fold'] != fold].index\n",
        "    val_idx = folds[folds['fold'] == fold].index\n",
        "\n",
        "    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
        "    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
        "    valid_labels = valid_folds[CFG.target_col].values\n",
        "\n",
        "    train_dataset = TrainDataset(train_folds, \n",
        "                                 transform=get_transforms(data='train'))\n",
        "    valid_dataset = TrainDataset(valid_folds, \n",
        "                                 transform=get_transforms(data='valid'))\n",
        "    train_loader = DataLoader(train_dataset, \n",
        "                              batch_size=CFG.batch_size, \n",
        "                              shuffle=True, \n",
        "                              num_workers=CFG.num_workers, pin_memory=True,\n",
        "                              drop_last=True, worker_init_fn=worker_init_fn)\n",
        "    valid_loader = DataLoader(valid_dataset, \n",
        "                              batch_size=CFG.batch_size * 2, \n",
        "                              shuffle=False, \n",
        "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
        "    # ====================================================\n",
        "    # optimizer\n",
        "    # ====================================================\n",
        "    def get_optimizer(lr=CFG.lr):\n",
        "        if CFG.optimizer == 'adam':\n",
        "            optimizer = Adam(model.parameters(), lr=lr, weight_decay=CFG.weight_decay, amsgrad=False)\n",
        "        elif CFG.optimizer == 'adamw':\n",
        "            optimizer = AdamW(model.parameters(), lr=lr, weight_decay=CFG.weight_decay)\n",
        "        return optimizer\n",
        "    \n",
        "    # ====================================================\n",
        "    # criterion\n",
        "    # ====================================================\n",
        "    def get_criterion():\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        return criterion\n",
        "    \n",
        "    # ====================================================\n",
        "    # scheduler\n",
        "    # ====================================================\n",
        "    def get_scheduler(optimizer):\n",
        "        if CFG.scheduler=='ReduceLROnPlateau':\n",
        "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n",
        "        elif CFG.scheduler=='CosineAnnealingLR':\n",
        "            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n",
        "        if CFG.warmup_epochs > 0:\n",
        "            scheduler = GradualWarmupSchedulerV2(optimizer, multiplier=CFG.multiplier,\n",
        "                                                 total_epoch=CFG.warmup_epochs, \n",
        "                                                 after_scheduler=scheduler)\n",
        "        return scheduler \n",
        "    \n",
        "    # ====================================================\n",
        "    # training loop\n",
        "    # ====================================================\n",
        "    model = CustomModel(CFG, pretrained=True)\n",
        "    model.to(device)\n",
        "    \n",
        "    optimizer = get_optimizer()\n",
        "    scheduler = get_scheduler(optimizer)\n",
        "    criterion = get_criterion()\n",
        "\n",
        "    best_score = 0.\n",
        "    best_loss = np.inf\n",
        "\n",
        "    for epoch in range(CFG.epochs + CFG.epoch_no_aug):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # train\n",
        "        if epoch < CFG.epochs:\n",
        "            avg_loss = train_fn(train_loader, model, criterion, optimizer,\n",
        "                                 epoch, scheduler,\n",
        "                                 device,\n",
        "                                 mode=CFG.aug_mode)\n",
        "        else:\n",
        "            optimizer = get_optimizer(lr=1e-6)\n",
        "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n",
        "            avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, mode='none')\n",
        "\n",
        "        # eval\n",
        "        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
        "    \n",
        "        if isinstance(scheduler, ReduceLROnPlateau):\n",
        "            scheduler.step(avg_val_loss)\n",
        "        elif isinstance(scheduler, CosineAnnealingLR):\n",
        "            scheduler.step()\n",
        "        elif isinstance(scheduler, GradualWarmupSchedulerV2):\n",
        "            scheduler.step()\n",
        "\n",
        "        # scoring\n",
        "        score = get_score(valid_labels, preds)\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
        "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
        "            torch.save({'model': model.state_dict(), \n",
        "                        'preds': preds},\n",
        "                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_score.pth')\n",
        "        \n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            LOGGER.info(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
        "            torch.save({'model': model.state_dict(), \n",
        "                        'preds': preds},\n",
        "                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_loss.pth')\n",
        "    \n",
        "    valid_folds['preds'] = torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_loss.pth', \n",
        "                                      map_location=torch.device('cpu'))['preds']\n",
        "\n",
        "    return valid_folds"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkFH8gWHTBfV"
      },
      "source": [
        "# ====================================================\n",
        "# main\n",
        "# ====================================================\n",
        "def main():\n",
        "\n",
        "    \"\"\"\n",
        "    Prepare: 1.train \n",
        "    \"\"\"\n",
        "\n",
        "    def get_result(result_df):\n",
        "        preds = result_df['preds'].values\n",
        "        labels = result_df[CFG.target_col].values\n",
        "        score = get_score(labels, preds)\n",
        "        LOGGER.info(f'Score: {score:<.4f}')\n",
        "    \n",
        "    if CFG.train:\n",
        "        # train \n",
        "        oof_df = pd.DataFrame()\n",
        "        for fold in range(CFG.n_fold):\n",
        "            if fold in CFG.trn_folds:\n",
        "                _oof_df = train_loop(train, fold)\n",
        "                oof_df = pd.concat([oof_df, _oof_df])\n",
        "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
        "                get_result(_oof_df)\n",
        "        # CV result\n",
        "        LOGGER.info(f\"========== CV ==========\")\n",
        "        get_result(oof_df)\n",
        "        # save result\n",
        "        oof_df.to_csv(OUTPUT_DIR+'oof_df.csv', index=False)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN-OoYUCf-0-"
      },
      "source": [
        "#! wget https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b1-533bc792.pth"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08LyFXQpRR2-"
      },
      "source": [
        "#!mkdir -p /root/.cache/torch/hub/checkpoints/"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w0JJBGUf-7c"
      },
      "source": [
        "#!cp efficientnet_b1-533bc792.pth  /root/.cache/torch/hub/checkpoints/efficientnet_b1-533bc792.pth"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebNYN9ybTBfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6168b272-3378-475b-a28c-862c04ba0300"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========== fold: 0 training ==========\n",
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b2_ra-bcdf34b7.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_ra-bcdf34b7.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [1][0/1500] Data 92.512 (92.512) Elapsed 1m 33s (remain 2335m 31s) Loss: 0.7077(0.7077) Grad: 2.8420  \n",
            "Epoch: [1][100/1500] Data 0.000 (1.522) Elapsed 3m 58s (remain 54m 58s) Loss: 0.1862(0.3908) Grad: 1.1635  \n",
            "Epoch: [1][200/1500] Data 0.000 (1.055) Elapsed 6m 20s (remain 41m 2s) Loss: 0.3370(0.3581) Grad: 2.4769  \n",
            "Epoch: [1][300/1500] Data 0.000 (0.899) Elapsed 8m 44s (remain 34m 47s) Loss: 0.2697(0.3472) Grad: 1.4493  \n",
            "Epoch: [1][400/1500] Data 0.000 (0.806) Elapsed 11m 1s (remain 30m 13s) Loss: 0.6307(0.3453) Grad: 2.4283  \n",
            "Epoch: [1][500/1500] Data 0.000 (0.757) Elapsed 13m 22s (remain 26m 40s) Loss: 0.2473(0.3427) Grad: 0.9417  \n",
            "Epoch: [1][600/1500] Data 0.000 (0.726) Elapsed 15m 44s (remain 23m 33s) Loss: 0.1585(0.3389) Grad: 0.8139  \n",
            "Epoch: [1][700/1500] Data 0.000 (0.707) Elapsed 18m 8s (remain 20m 41s) Loss: 0.4569(0.3382) Grad: 1.3436  \n",
            "Epoch: [1][800/1500] Data 0.000 (0.730) Elapsed 21m 3s (remain 18m 22s) Loss: 0.3025(0.3366) Grad: 0.7509  \n",
            "Epoch: [1][900/1500] Data 0.000 (0.778) Elapsed 24m 23s (remain 16m 12s) Loss: 0.6156(0.3361) Grad: 1.5305  \n",
            "Epoch: [1][1000/1500] Data 0.000 (0.783) Elapsed 27m 10s (remain 13m 32s) Loss: 0.4477(0.3367) Grad: 0.9522  \n",
            "Epoch: [1][1100/1500] Data 0.000 (0.778) Elapsed 29m 48s (remain 10m 48s) Loss: 0.3128(0.3352) Grad: 0.8265  \n",
            "Epoch: [1][1200/1500] Data 0.000 (0.762) Elapsed 32m 11s (remain 8m 0s) Loss: 0.3643(0.3345) Grad: 0.8955  \n",
            "Epoch: [1][1300/1500] Data 3.022 (0.746) Elapsed 34m 31s (remain 5m 16s) Loss: 0.3912(0.3346) Grad: 0.8088  \n",
            "Epoch: [1][1400/1500] Data 0.000 (0.732) Elapsed 36m 52s (remain 2m 36s) Loss: 0.3780(0.3346) Grad: 0.9047  \n",
            "Epoch: [1][1499/1500] Data 0.000 (0.719) Elapsed 39m 9s (remain 0m 0s) Loss: 0.5129(0.3328) Grad: 1.1390  \n",
            "EVAL: [0/188] Data 21.149 (21.149) Elapsed 0m 21s (remain 67m 15s) Loss: 0.3507(0.3507) \n",
            "EVAL: [100/188] Data 0.000 (2.252) Elapsed 4m 24s (remain 3m 47s) Loss: 0.2856(0.3266) \n",
            "EVAL: [187/188] Data 0.000 (2.612) Elapsed 9m 20s (remain 0m 0s) Loss: 0.1109(0.3260) \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 - avg_train_loss: 0.3328  avg_val_loss: 0.3260  time: 2911s\n",
            "Epoch 1 - Score: 0.5156\n",
            "Epoch 1 - Save Best Score: 0.5156 Model\n",
            "Epoch 1 - Save Best Loss: 0.3260 Model\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [2][0/1500] Data 13.685 (13.685) Elapsed 0m 14s (remain 371m 32s) Loss: 0.3867(0.3867) Grad: 0.8007  \n",
            "Epoch: [2][100/1500] Data 0.000 (0.794) Elapsed 2m 46s (remain 38m 28s) Loss: 0.3821(0.3163) Grad: 0.5388  \n",
            "Epoch: [2][200/1500] Data 0.431 (0.684) Elapsed 5m 9s (remain 33m 20s) Loss: 0.3811(0.3279) Grad: 0.2880  \n",
            "Epoch: [2][300/1500] Data 0.000 (0.975) Elapsed 9m 10s (remain 36m 33s) Loss: 0.3810(0.3250) Grad: 0.4043  \n",
            "Epoch: [2][400/1500] Data 0.000 (0.983) Elapsed 12m 16s (remain 33m 38s) Loss: 0.0795(0.3233) Grad: 0.4849  \n",
            "Epoch: [2][500/1500] Data 0.001 (0.962) Elapsed 15m 9s (remain 30m 13s) Loss: 0.3108(0.3253) Grad: 0.2477  \n",
            "Epoch: [2][600/1500] Data 0.000 (0.898) Elapsed 17m 32s (remain 26m 14s) Loss: 0.2586(0.3248) Grad: 0.3353  \n",
            "Epoch: [2][700/1500] Data 0.000 (0.842) Elapsed 19m 48s (remain 22m 34s) Loss: 0.2477(0.3228) Grad: 0.2933  \n",
            "Epoch: [2][800/1500] Data 0.000 (0.788) Elapsed 21m 53s (remain 19m 5s) Loss: 0.3825(0.3239) Grad: 0.2113  \n",
            "Epoch: [2][900/1500] Data 0.000 (0.777) Elapsed 24m 24s (remain 16m 13s) Loss: 0.3113(0.3253) Grad: 0.1782  \n",
            "Epoch: [2][1000/1500] Data 7.308 (0.817) Elapsed 27m 43s (remain 13m 49s) Loss: 0.3698(0.3239) Grad: 0.3567  \n",
            "Epoch: [2][1100/1500] Data 0.000 (0.817) Elapsed 30m 27s (remain 11m 2s) Loss: 0.2371(0.3246) Grad: 0.2995  \n",
            "Epoch: [2][1200/1500] Data 15.439 (0.834) Elapsed 33m 30s (remain 8m 20s) Loss: 0.3903(0.3243) Grad: 0.8068  \n",
            "Epoch: [2][1300/1500] Data 0.000 (0.891) Elapsed 37m 28s (remain 5m 43s) Loss: 0.1660(0.3236) Grad: 0.3652  \n",
            "Epoch: [2][1400/1500] Data 3.724 (0.911) Elapsed 40m 47s (remain 2m 52s) Loss: 0.4721(0.3247) Grad: 0.7950  \n",
            "Epoch: [2][1499/1500] Data 0.000 (0.903) Elapsed 43m 26s (remain 0m 0s) Loss: 0.5413(0.3251) Grad: 0.7802  \n",
            "EVAL: [0/188] Data 21.674 (21.674) Elapsed 0m 22s (remain 68m 47s) Loss: 0.3222(0.3222) \n",
            "EVAL: [100/188] Data 0.000 (2.811) Elapsed 5m 19s (remain 4m 35s) Loss: 0.2426(0.2858) \n",
            "EVAL: [187/188] Data 0.000 (4.060) Elapsed 13m 51s (remain 0m 0s) Loss: 0.0835(0.2812) \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2 - avg_train_loss: 0.3251  avg_val_loss: 0.2812  time: 3438s\n",
            "Epoch 2 - Score: 0.7258\n",
            "Epoch 2 - Save Best Score: 0.7258 Model\n",
            "Epoch 2 - Save Best Loss: 0.2812 Model\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [3][0/1500] Data 19.926 (19.926) Elapsed 0m 21s (remain 527m 29s) Loss: 0.4062(0.4062) Grad: 0.5296  \n",
            "Epoch: [3][100/1500] Data 0.000 (1.413) Elapsed 3m 48s (remain 52m 51s) Loss: 0.3261(0.2982) Grad: 0.2298  \n",
            "Epoch: [3][200/1500] Data 0.000 (1.197) Elapsed 6m 52s (remain 44m 25s) Loss: 0.3903(0.3078) Grad: 0.4355  \n",
            "Epoch: [3][300/1500] Data 0.000 (1.110) Elapsed 9m 50s (remain 39m 12s) Loss: 0.2318(0.3099) Grad: 0.5113  \n",
            "Epoch: [3][400/1500] Data 0.000 (1.236) Elapsed 13m 57s (remain 38m 14s) Loss: 0.1669(0.3121) Grad: 0.3928  \n",
            "Epoch: [3][500/1500] Data 0.000 (1.224) Elapsed 17m 20s (remain 34m 33s) Loss: 0.3351(0.3093) Grad: 0.2230  \n",
            "Epoch: [3][600/1500] Data 0.000 (1.147) Elapsed 20m 1s (remain 29m 57s) Loss: 0.2359(0.3084) Grad: 0.3270  \n",
            "Epoch: [3][700/1500] Data 0.000 (1.088) Elapsed 22m 40s (remain 25m 50s) Loss: 0.1729(0.3087) Grad: 0.5378  \n",
            "Epoch: [3][800/1500] Data 0.000 (1.020) Elapsed 25m 0s (remain 21m 49s) Loss: 0.1470(0.3096) Grad: 0.4867  \n",
            "Epoch: [3][900/1500] Data 0.000 (0.957) Elapsed 27m 11s (remain 18m 4s) Loss: 0.6289(0.3069) Grad: 1.3123  \n",
            "Epoch: [3][1000/1500] Data 0.000 (0.900) Elapsed 29m 14s (remain 14m 34s) Loss: 0.1240(0.3060) Grad: 0.4858  \n",
            "Epoch: [3][1100/1500] Data 0.000 (0.853) Elapsed 31m 18s (remain 11m 20s) Loss: 0.2417(0.3050) Grad: 0.2468  \n",
            "Epoch: [3][1200/1500] Data 0.000 (0.806) Elapsed 33m 14s (remain 8m 16s) Loss: 0.2831(0.3049) Grad: 0.2587  \n",
            "Epoch: [3][1300/1500] Data 0.000 (0.770) Elapsed 35m 12s (remain 5m 23s) Loss: 0.3988(0.3050) Grad: 0.8796  \n",
            "Epoch: [3][1400/1500] Data 0.005 (0.734) Elapsed 37m 5s (remain 2m 37s) Loss: 0.5237(0.3064) Grad: 0.6299  \n",
            "Epoch: [3][1499/1500] Data 0.000 (0.707) Elapsed 39m 2s (remain 0m 0s) Loss: 0.3416(0.3080) Grad: 0.2988  \n",
            "EVAL: [0/188] Data 17.295 (17.295) Elapsed 0m 17s (remain 55m 10s) Loss: 0.2652(0.2652) \n",
            "EVAL: [100/188] Data 0.001 (1.807) Elapsed 3m 38s (remain 3m 8s) Loss: 0.2616(0.2824) \n",
            "EVAL: [187/188] Data 0.000 (2.161) Elapsed 7m 54s (remain 0m 0s) Loss: 0.1219(0.2812) \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3 - avg_train_loss: 0.3080  avg_val_loss: 0.2812  time: 2818s\n",
            "Epoch 3 - Score: 0.7314\n",
            "Epoch 3 - Save Best Score: 0.7314 Model\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [4][0/1500] Data 12.315 (12.315) Elapsed 0m 13s (remain 339m 26s) Loss: 0.2557(0.2557) Grad: 0.3955  \n",
            "Epoch: [4][100/1500] Data 0.000 (0.616) Elapsed 2m 29s (remain 34m 25s) Loss: 0.3747(0.3090) Grad: 0.3282  \n",
            "Epoch: [4][200/1500] Data 0.000 (0.513) Elapsed 4m 35s (remain 29m 41s) Loss: 0.1397(0.3001) Grad: 0.4172  \n",
            "Epoch: [4][300/1500] Data 0.000 (0.464) Elapsed 6m 38s (remain 26m 27s) Loss: 0.3105(0.3001) Grad: 0.3615  \n",
            "Epoch: [4][400/1500] Data 0.000 (0.442) Elapsed 8m 41s (remain 23m 47s) Loss: 0.2865(0.2991) Grad: 0.5477  \n",
            "Epoch: [4][500/1500] Data 0.000 (0.425) Elapsed 10m 42s (remain 21m 20s) Loss: 0.2205(0.2979) Grad: 0.3192  \n",
            "Epoch: [4][600/1500] Data 0.000 (0.400) Elapsed 12m 35s (remain 18m 50s) Loss: 0.0734(0.2966) Grad: 0.3700  \n",
            "Epoch: [4][700/1500] Data 0.000 (0.417) Elapsed 14m 53s (remain 16m 58s) Loss: 0.2418(0.2972) Grad: 0.2741  \n",
            "Epoch: [4][800/1500] Data 0.002 (0.460) Elapsed 17m 34s (remain 15m 20s) Loss: 0.1401(0.2972) Grad: 0.4021  \n",
            "Epoch: [4][900/1500] Data 0.000 (0.482) Elapsed 20m 4s (remain 13m 21s) Loss: 0.3101(0.2975) Grad: 0.2291  \n",
            "Epoch: [4][1000/1500] Data 0.000 (0.496) Elapsed 22m 32s (remain 11m 14s) Loss: 0.2707(0.2969) Grad: 0.2539  \n",
            "Epoch: [4][1100/1500] Data 0.000 (0.573) Elapsed 26m 12s (remain 9m 29s) Loss: 0.1311(0.2956) Grad: 0.5746  \n",
            "Epoch: [4][1200/1500] Data 0.000 (0.613) Elapsed 29m 22s (remain 7m 18s) Loss: 0.1444(0.2952) Grad: 0.5055  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMDIy-Y-TBfW"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9ZVL6rXTBfW"
      },
      "source": [
        "models = get_models(CFG, mode='loss')\n",
        "models_ = get_models(CFG, mode='score')\n",
        "test_dataset = TestDataset(test, transform=get_transforms(data='valid'))\n",
        "test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size*2, shuffle=False, \n",
        "                         num_workers=CFG.num_workers, pin_memory=True)\n",
        "fold_preds = flip_inference(models, test_loader, device)\n",
        "fold_preds_ = flip_inference(models_, test_loader, device)\n",
        "preds = np.mean(fold_preds, axis=0)\n",
        "preds_ = np.mean(fold_preds_, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsABuF7yTBfW"
      },
      "source": [
        "test = test[['id', 'target']]\n",
        "test[CFG.target_col] = (preds + preds_) / 2\n",
        "test.to_csv(OUTPUT_DIR+'submission.csv', index=False)\n",
        "display(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoEqzbfZeylq"
      },
      "source": [
        "!cp submission.csv  /content/drive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}