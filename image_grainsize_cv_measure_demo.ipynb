{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_grainsize_cv_measure_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNag1bXa8nYn1o3C4jLFMLW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flydragon2018/colab_notebooks/blob/main/image_grainsize_cv_measure_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xohsVHyi_vve"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2isS2vi_2YH"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbDS4EIJ_2b2"
      },
      "source": [
        "! unzip -q  /content/drive/MyDrive/hw/2021hwjx-data-train.zip -d /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygoPK2k7_wou"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0XettVZ-F6K"
      },
      "source": [
        "g_avgarea=[0.2581,0.1290,0.0912,0.0645,0.0456,0.0323,0.0228,0.0161,0.0114,0.00806,0.00570,0.00403,0.00285,0.00202,0.00143,0.00101,0.00071,0.00050,0.00036,0.00025,0.00018,0.00013,0.000089,0.000063,0.000045,0.000032,0.000022,0.000016,0.000011,0.000008]\n",
        "g_avgdiameter=[0.5080,0.3592,0.3021,0.2540,0.2136,0.1796,0.1510,0.1270,0.1068,0.0898,0.0755,0.0635,0.0534,0.0449,0.0378,0.0318,0.0267,0.0225,0.0186,0.0159,0.0133,0.0112,0.0094,0.0079,0.0067,0.0056,0.0047,0.0040,0.0033,0.0028]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxPrGXlf-fJk"
      },
      "source": [
        "#g_names=['00','0','0.5','1.0','1.5','2.0','2.5','3.0','3.5','4.0','5.0','5.5','6.0','6.5','7.0','7.5','8.0','8.5','9.0','9.5','10.0','10.5','11.0','11.5','12.0','12.5','13.0','13.5','14.0']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKAEX77H-fNu"
      },
      "source": [
        "g_names=['6.5','7.0','7.5','8.0','8.5','9.0','9.5','10.0','10.5','11.0','11.5','12.0','12.5','13.0']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHs5QHaN-fRO"
      },
      "source": [
        "g_areas=[0.00143,0.00101,0.00071,0.00050,0.00036,0.00025,0.00018,0.00013,0.000089,0.000063,0.000045,0.000032,0.000022,0.000016]\n",
        "g_diameters=[0.0378,0.0318,0.0267,0.0225,0.0186,0.0159,0.0133,0.0112,0.0094,0.0079,0.0067,0.0056,0.0047,0.0040]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQQpOijIeLJa"
      },
      "source": [
        "##   G=-6.643856*lg(avg_diameter)-3.288"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D7p1x0deLMr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70XoMeYq-fVJ"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage import io, color, measure \n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "uSXxQqT5-fYy",
        "outputId": "3d3469a9-014a-4229-e600-0fd970487338"
      },
      "source": [
        "'''\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from model_service.pytorch_model_service import PTServingBaseService\n",
        "\n",
        "import time\n",
        "from metric.metrics_manager import MetricsManager\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "\n",
        "import log\n",
        "from PIL import Image\n",
        "\n",
        "Image.MAX_IMAGE_PIXELS = 1000000000000000\n",
        "logger = log.getLogger(__name__)\n",
        "\n",
        "logger.info(torch.__version__)\n",
        "logger.info(torchvision.__version__)\n",
        "\n",
        "\n",
        "class ImageClassificationService(PTServingBaseService):\n",
        "    def __init__(self, model_name, model_path, **kwargs):\n",
        "        self.model_name = model_name\n",
        "        self.model_path = model_path\n",
        "        self.labels = self.read_classes()\n",
        "        self.num_classes = len(self.labels)\n",
        "        logger.info('{}-{}'.format(self.num_classes, self.model_path))\n",
        "        for key in kwargs:\n",
        "            logger.info('{}-{}'.format(key, kwargs[key]))\n",
        "\n",
        "        self.model = self.initial_model(self.num_classes)\n",
        "\n",
        "        self.use_cuda = False\n",
        "        self.device = torch.device(\"cuda\"\n",
        "                                   if torch.cuda.is_available() else \"cpu\")\n",
        "        if torch.cuda.is_available():\n",
        "            logger.info('Using GPU for inference')\n",
        "            self.model.to(self.device)\n",
        "            self.model.load_state_dict(torch.load(self.model_path)['model'])\n",
        "        else:\n",
        "            logger.info('Using CPU for inference')\n",
        "            self.model.load_state_dict(torch.load(self.model_path,\n",
        "                                                  map_location='cpu')['model'])\n",
        "        self.model.eval()\n",
        "\n",
        "    def initial_model(self, num_classes):\n",
        "        model = torchvision.models.__dict__[\"resnet50\"](pretrained=False)\n",
        "        channel_in = model.fc.in_features\n",
        "        model.fc = nn.Linear(channel_in, num_classes)\n",
        "        logger.info('{}-{}'.format(channel_in, num_classes))\n",
        "        return model\n",
        "\n",
        "    # transform img before inference\n",
        "    def transform_img(self, img):\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                         std=[0.229, 0.224, 0.225])\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            normalize])\n",
        "        return transform(img)\n",
        "\n",
        "    def read_classes(self):\n",
        "        labels = []\n",
        "        class_path = os.path.join(os.path.dirname(self.model_path), 'classes.txt')\n",
        "        with open(class_path) as lines:\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                labels.append(line)\n",
        "        return labels\n",
        "\n",
        "    def _preprocess(self, data):\n",
        "        preprocessed_data = {}\n",
        "        for k, v in data.items():\n",
        "            for file_name, file_content in v.items():\n",
        "                img = Image.open(file_content).convert(\"RGB\")\n",
        "                img = self.transform_img(img)\n",
        "                preprocessed_data[k] = torch.unsqueeze(img, dim=0).to(self.device)\n",
        "        return preprocessed_data\n",
        "\n",
        "    def _inference(self, data):\n",
        "        img = data[\"input_img\"]\n",
        "        data = img\n",
        "        res = self.model(data)\n",
        "        res_softmax = F.softmax(res[0],dim=0).to('cpu')\n",
        "        conf, pred = res_softmax.topk(1, 0, True, True)\n",
        "        label = self.labels[pred.item()]\n",
        "        result = {}\n",
        "        result['label'] = label\n",
        "        result['confidence'] = float('{0:.4f}'.format(conf.item()))\n",
        "        result = {\"result\": result}\n",
        "        return result\n",
        "\n",
        "    def _postprocess(self, data):\n",
        "        return data\n",
        "\n",
        "    def inference(self, data):\n",
        "        pre_start_time = time.time()\n",
        "        data = self._preprocess(data)\n",
        "        infer_start_time = time.time()\n",
        "        # Update preprocess latency metric\n",
        "        pre_time_in_ms = (infer_start_time - pre_start_time) * 1000\n",
        "        logger.info('preprocess time: ' + str(pre_time_in_ms) + 'ms')\n",
        "        if self.model_name + '_LatencyPreprocess' in MetricsManager.metrics:\n",
        "            MetricsManager.metrics[self.model_name + '_LatencyPreprocess'].update(pre_time_in_ms)\n",
        "        data = self._inference(data)\n",
        "        infer_end_time = time.time()\n",
        "        infer_in_ms = (infer_end_time - infer_start_time) * 1000\n",
        "        logger.info('infer time: ' + str(infer_in_ms) + 'ms')\n",
        "        data = self._postprocess(data)\n",
        "        # Update inference latency metric\n",
        "        post_time_in_ms = (time.time() - infer_end_time) * 1000\n",
        "        logger.info('postprocess time: ' + str(post_time_in_ms) + 'ms')\n",
        "        if self.model_name + '_LatencyInference' in MetricsManager.metrics:\n",
        "            MetricsManager.metrics[self.model_name + '_LatencyInference'].update(post_time_in_ms)\n",
        "        # Update overall latency metric\n",
        "        if self.model_name + '_LatencyOverall' in MetricsManager.metrics:\n",
        "            MetricsManager.metrics[self.model_name + '_LatencyOverall'].update(pre_time_in_ms + post_time_in_ms)\n",
        "        logger.info('latency: ' + str(pre_time_in_ms + infer_in_ms + post_time_in_ms) + 'ms')\n",
        "        data['latency_time'] = pre_time_in_ms + infer_in_ms + post_time_in_ms\n",
        "        return data\n",
        "\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n# -*- coding: utf-8 -*-\\nimport os\\nimport torch\\nimport torchvision\\n\\nfrom model_service.pytorch_model_service import PTServingBaseService\\n\\nimport time\\nfrom metric.metrics_manager import MetricsManager\\nfrom torch import nn\\nimport torch.nn.functional as F\\nfrom torchvision import transforms\\n\\nimport log\\nfrom PIL import Image\\n\\nImage.MAX_IMAGE_PIXELS = 1000000000000000\\nlogger = log.getLogger(__name__)\\n\\nlogger.info(torch.__version__)\\nlogger.info(torchvision.__version__)\\n\\n\\nclass ImageClassificationService(PTServingBaseService):\\n    def __init__(self, model_name, model_path, **kwargs):\\n        self.model_name = model_name\\n        self.model_path = model_path\\n        self.labels = self.read_classes()\\n        self.num_classes = len(self.labels)\\n        logger.info(\\'{}-{}\\'.format(self.num_classes, self.model_path))\\n        for key in kwargs:\\n            logger.info(\\'{}-{}\\'.format(key, kwargs[key]))\\n\\n        self.model = self.initial_model(self.num_classes)\\n\\n        self.use_cuda = False\\n        self.device = torch.device(\"cuda\"\\n                                   if torch.cuda.is_available() else \"cpu\")\\n        if torch.cuda.is_available():\\n            logger.info(\\'Using GPU for inference\\')\\n            self.model.to(self.device)\\n            self.model.load_state_dict(torch.load(self.model_path)[\\'model\\'])\\n        else:\\n            logger.info(\\'Using CPU for inference\\')\\n            self.model.load_state_dict(torch.load(self.model_path,\\n                                                  map_location=\\'cpu\\')[\\'model\\'])\\n        self.model.eval()\\n\\n    def initial_model(self, num_classes):\\n        model = torchvision.models.__dict__[\"resnet50\"](pretrained=False)\\n        channel_in = model.fc.in_features\\n        model.fc = nn.Linear(channel_in, num_classes)\\n        logger.info(\\'{}-{}\\'.format(channel_in, num_classes))\\n        return model\\n\\n    # transform img before inference\\n    def transform_img(self, img):\\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\\n                                         std=[0.229, 0.224, 0.225])\\n        transform = transforms.Compose([\\n            transforms.Resize(256),\\n            transforms.CenterCrop(224),\\n            transforms.ToTensor(),\\n            normalize])\\n        return transform(img)\\n\\n    def read_classes(self):\\n        labels = []\\n        class_path = os.path.join(os.path.dirname(self.model_path), \\'classes.txt\\')\\n        with open(class_path) as lines:\\n            for line in lines:\\n                line = line.strip()\\n                labels.append(line)\\n        return labels\\n\\n    def _preprocess(self, data):\\n        preprocessed_data = {}\\n        for k, v in data.items():\\n            for file_name, file_content in v.items():\\n                img = Image.open(file_content).convert(\"RGB\")\\n                img = self.transform_img(img)\\n                preprocessed_data[k] = torch.unsqueeze(img, dim=0).to(self.device)\\n        return preprocessed_data\\n\\n    def _inference(self, data):\\n        img = data[\"input_img\"]\\n        data = img\\n        res = self.model(data)\\n        res_softmax = F.softmax(res[0],dim=0).to(\\'cpu\\')\\n        conf, pred = res_softmax.topk(1, 0, True, True)\\n        label = self.labels[pred.item()]\\n        result = {}\\n        result[\\'label\\'] = label\\n        result[\\'confidence\\'] = float(\\'{0:.4f}\\'.format(conf.item()))\\n        result = {\"result\": result}\\n        return result\\n\\n    def _postprocess(self, data):\\n        return data\\n\\n    def inference(self, data):\\n        pre_start_time = time.time()\\n        data = self._preprocess(data)\\n        infer_start_time = time.time()\\n        # Update preprocess latency metric\\n        pre_time_in_ms = (infer_start_time - pre_start_time) * 1000\\n        logger.info(\\'preprocess time: \\' + str(pre_time_in_ms) + \\'ms\\')\\n        if self.model_name + \\'_LatencyPreprocess\\' in MetricsManager.metrics:\\n            MetricsManager.metrics[self.model_name + \\'_LatencyPreprocess\\'].update(pre_time_in_ms)\\n        data = self._inference(data)\\n        infer_end_time = time.time()\\n        infer_in_ms = (infer_end_time - infer_start_time) * 1000\\n        logger.info(\\'infer time: \\' + str(infer_in_ms) + \\'ms\\')\\n        data = self._postprocess(data)\\n        # Update inference latency metric\\n        post_time_in_ms = (time.time() - infer_end_time) * 1000\\n        logger.info(\\'postprocess time: \\' + str(post_time_in_ms) + \\'ms\\')\\n        if self.model_name + \\'_LatencyInference\\' in MetricsManager.metrics:\\n            MetricsManager.metrics[self.model_name + \\'_LatencyInference\\'].update(post_time_in_ms)\\n        # Update overall latency metric\\n        if self.model_name + \\'_LatencyOverall\\' in MetricsManager.metrics:\\n            MetricsManager.metrics[self.model_name + \\'_LatencyOverall\\'].update(pre_time_in_ms + post_time_in_ms)\\n        logger.info(\\'latency: \\' + str(pre_time_in_ms + infer_in_ms + post_time_in_ms) + \\'ms\\')\\n        data[\\'latency_time\\'] = pre_time_in_ms + infer_in_ms + post_time_in_ms\\n        return data\\n\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1pBJJNK_--W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2qqgwwt__FD"
      },
      "source": [
        "def __init__(self, model_name, model_path, **kwargs):\n",
        "    self.model_name = model_name\n",
        "    self.model_path = model_path\n",
        "    self.labels = self.read_classes()\n",
        "    self.num_classes = len(self.labels)\n",
        "    logger.info('{}-{}'.format(self.num_classes, self.model_path))\n",
        "    for key in kwargs:\n",
        "        logger.info('{}-{}'.format(key, kwargs[key]))\n",
        "\n",
        "    self.model = self.initial_model(self.num_classes)\n",
        "\n",
        "    self.use_cuda = False\n",
        "    self.device = torch.device(\"cuda\"\n",
        "                               if torch.cuda.is_available() else \"cpu\")\n",
        "    if torch.cuda.is_available():\n",
        "        logger.info('Using GPU for inference')\n",
        "        self.model.to(self.device)\n",
        "        self.model.load_state_dict(torch.load(self.model_path)['model'])\n",
        "    else:\n",
        "        logger.info('Using CPU for inference')\n",
        "        self.model.load_state_dict(torch.load(self.model_path,\n",
        "                                              map_location='cpu')['model'])\n",
        "    self.model.eval()\n",
        "\n",
        "def initial_model(self, num_classes):\n",
        "    model = torchvision.models.__dict__[\"resnet50\"](pretrained=False)\n",
        "    channel_in = model.fc.in_features\n",
        "    model.fc = nn.Linear(channel_in, num_classes)\n",
        "    logger.info('{}-{}'.format(channel_in, num_classes))\n",
        "    return model\n",
        "\n",
        "# transform img before inference\n",
        "def transform_img(self, img):\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        normalize])\n",
        "    return transform(img)\n",
        "\n",
        "def read_classes(self):\n",
        "    labels = []\n",
        "    class_path = os.path.join(os.path.dirname(self.model_path), 'classes.txt')\n",
        "    with open(class_path) as lines:\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            labels.append(line)\n",
        "    return labels\n",
        "\n",
        "def _preprocess(self, data):\n",
        "    preprocessed_data = {}\n",
        "    for k, v in data.items():\n",
        "        for file_name, file_content in v.items():\n",
        "            img = Image.open(file_content).convert(\"RGB\")\n",
        "            img = self.transform_img(img)\n",
        "            preprocessed_data[k] = torch.unsqueeze(img, dim=0).to(self.device)\n",
        "    return preprocessed_data\n",
        "\n",
        "def _inference(self, data):\n",
        "    img = data[\"input_img\"]\n",
        "    data = img\n",
        "    #res = self.model(data)\n",
        "    #res_softmax = F.softmax(res[0],dim=0).to('cpu')\n",
        "    #conf, pred = res_softmax.topk(1, 0, True, True)\n",
        "    #label = self.labels[pred.item()]\n",
        "    labels=['6.5','7.0','7.5','8.0','8.5','9.0','9.5','10.0','10.5','11.0','11.5','12.0','12.5','13.0']\n",
        "    pix2mm=25.4/96\n",
        "    M=500\n",
        "    img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    ret,thresh=cv2.threshold(img,0,255,cv2.THRESH_OTSU)\n",
        "    kernel=np.ones((3,3),np.uint8)\n",
        "    eroded=cv2.erode(thresh,kernel,iterations=5)\n",
        "    dilated=cv2.dilate(eroded,kernel,iterations=2)  \n",
        "\n",
        "    mask=dilated==255\n",
        "    s=[[1,1,1],[1,1,1],[1,1,1]]\n",
        "    \n",
        "\n",
        "    labeled_mask,num_labels=ndimage.label(mask,structure=s)\n",
        "    clusters=measure.regionprops(labeled_mask)\n",
        "    d=[]\n",
        "    for props in clusters:\n",
        "      d.append(props['equivalent_diameter'])\n",
        "    md=np.median(d)\n",
        "    gmd=md*pix2mm/M\n",
        "    err_d=list(abs(g_diameters-gmd))\n",
        "    idx=err_d.index(np.min(err_d))\n",
        "\n",
        "\n",
        "    result = {}\n",
        "    result['label'] = labels[idx]\n",
        "    #result['confidence'] = float('{0:.4f}'.format(conf.item()))\n",
        "    result = {\"result\": result}\n",
        "    return result\n",
        "\n",
        "def _postprocess(self, data):\n",
        "    return data\n",
        "\n",
        "def inference(self, data):\n",
        "    pre_start_time = time.time()\n",
        "    data = self._preprocess(data)\n",
        "    infer_start_time = time.time()\n",
        "    # Update preprocess latency metric\n",
        "    pre_time_in_ms = (infer_start_time - pre_start_time) * 1000\n",
        "    logger.info('preprocess time: ' + str(pre_time_in_ms) + 'ms')\n",
        "    if self.model_name + '_LatencyPreprocess' in MetricsManager.metrics:\n",
        "        MetricsManager.metrics[self.model_name + '_LatencyPreprocess'].update(pre_time_in_ms)\n",
        "    data = self._inference(data)\n",
        "    infer_end_time = time.time()\n",
        "    infer_in_ms = (infer_end_time - infer_start_time) * 1000\n",
        "    logger.info('infer time: ' + str(infer_in_ms) + 'ms')\n",
        "    data = self._postprocess(data)\n",
        "    # Update inference latency metric\n",
        "    post_time_in_ms = (time.time() - infer_end_time) * 1000\n",
        "    logger.info('postprocess time: ' + str(post_time_in_ms) + 'ms')\n",
        "    if self.model_name + '_LatencyInference' in MetricsManager.metrics:\n",
        "        MetricsManager.metrics[self.model_name + '_LatencyInference'].update(post_time_in_ms)\n",
        "    # Update overall latency metric\n",
        "    if self.model_name + '_LatencyOverall' in MetricsManager.metrics:\n",
        "        MetricsManager.metrics[self.model_name + '_LatencyOverall'].update(pre_time_in_ms + post_time_in_ms)\n",
        "    logger.info('latency: ' + str(pre_time_in_ms + infer_in_ms + post_time_in_ms) + 'ms')\n",
        "    data['latency_time'] = pre_time_in_ms + infer_in_ms + post_time_in_ms\n",
        "    return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZTMqJGR__IB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrKuvSev__K2"
      },
      "source": [
        "scipy.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFH2QBBN__OB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QxpfaU2__Q9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxTw_bUL__To"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFmjsT64__Wy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ualsUJCH__Zs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_DlwrTd__cx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}